---
title: "A Functional Approach to (parallelised) Monte Carlo Simulation"
subtitle: "Advanced R for Econometricians"
type: "Final Project"
author: "Alexander Langnau, Öcal Kaptan, Sunyoung Ji"
discipline: "M.Sc. Econometrics"
date: "today"
studid: "232907, 230914, 229979"
supervisor: "Prof. Dr. Christoph Hanck"
secondsupervisor: "M.Sc. Martin C. Arnold, M.Sc. Jens Klenke"
ssemester: "1"
estdegree_emester: "Summer Term 2022"
deadline: "09. 09. 2022"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 11pt
geometry: lmargin=2.5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
classoption: a4paper
language: english
---

<!-- % Template Version 1.2 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
```

```{r library, include=FALSE}
require(utils)
library(tidyverse)
library(purrr)
library(parallel)
library(furrr)
```

# Introduction


The Monte Carlo method is a simulation method for calculating the probabilistic value of the desired function using random numbers. A repeated pseudo-random number generator estimates sample statistics and returns a probability distribution of the sample, representing the parameter [@Barbu_2020]. Monte Carlo methods are combined with programming in modern research and contribute to various studies in statistics, economics and many other scientific fields. 
This paper progresses on developing a collection of different wrapper functions to partially automatize the process of running a Monte Carlo simulation. The Main function provides a convenient interface for Monte Carlo simulations and allows user to create a parameter grid and iterate homogeneous function calls over the parameter grid. It also offers informative summary statistics, including visualization with ggplot2-methods and an option to use a parallelisation plan using the `furrr` package.
The paper proceeds as follows: Chapter 2 describes pre-processes to establish the Monte Carlo simulation function. The pre-process contain functions to create a parameter grid and the respective data points drawn from a user-defined distribution, along with functions that provide summary statistics. Chapter 3 details the main Monte Carlo simulation function, which consists of the helper functions introduced in chapter 2. Chapter 4 presents specific examples for the simulation process. Finally, chapter 5 summarizes the work presented in this paper. 


# Pre-process: Creating Helper functions

The pre-process comprises 5 helper functions: `create_grid()`, `data_generation()`, `summary_function()`, `create_array_function()` and `output_function()`. The functions are named in a way that the underlying purpose is directly clear. These functions are the building blocks of the `main_function()`, which takes the user input and runs the Monte Carlo simulation by itself.

## create_grid()

The first helper function introduced is the `create_grid()`-function, which automatically creates a hyper-parameter grid over all permutations specified by the user.
  
  
```{r create grid}
create_grid <- function(parameters, nrep){
  input <- parameters
  storage <- list()
  name_vec <- c()
  
  for(i in 1:length(input)){ #1:3
    a <- as.numeric(input[[i]][[2]])
    b <- as.numeric(input[[i]][[3]])
    c <- as.numeric(input[[i]][[4]])
    output <- seq(from=a, to=b, by=c)
    storage[[i]] <-  output
    name_vec[i] <- input[[i]][[1]]
  }
  
  grid <- expand_grid(unlist(storage[1])
                      , unlist(storage[2])
                      , unlist(storage[3])
                      , unlist(storage[4])
                      , unlist(storage[5])
                      , c(1:nrep))
  
  names(grid) <- c(name_vec, "rep")
  
  return(grid)
}
```

Input for the parameter list has to follow a specific format as shown below:

```
parameter_list <- list(c("variable name 1", from, to, by) 
                      ,c("variable name 2", from, to, by)
                      ,c("variable name 3", from, to, by)
                      ,c("variable name 4", from, to, by))
```

`parameter_list` works with a minimum of 1 and a maximum of 4 variables. The structure of arguments is similar to `seq()` in R: Each vector contained in the list needs four arguments specified, that is, the function name, the start of the sequence, the end of the sequence and the steps, by which the interval gets divided. It would be easy to adapt this helper function for more parameters, but it is assumed that a grid with up to 4 parameters offers enough complexity for the simulation. The function basically takes the information of the input parameter list and creates a grid with  `tidyr::expand_grid()`. The argument `nrep` specifies how many repetitions per parameter constellation are created, where a separate row in the parameter grid gets created for each repetition.

**Example to `create_grid()`:**
```{r example create_grid}

#four parameters 
param_list0 <- list(c("n", 10, 20, 10)
                    ,c("mu", 0, 0.5, 0.25)
                    ,c("sd", 0, 0.3, 0.1)
                    ,c("gender", 0, 1, 1))

head(create_grid(param_list0, nrep=3), n=10)


```





## data_generation()

The second helper function called `data_generation()` takes the arguments `grid` and `simulation` as inputs. `simulation` is the argument for the user-defined function for the data generation process, while `grid` is the parameter grid previously created. The user can choose between a variety of probability distributions by entering the name of the function, for example, `rnorm()` for the normal distribution or `runif()` for the uniform distribution.

The n data points created for each set of parameters are stored as separate elements in a list since this format is a very flexible way of storing data. 

`data_generation()` chooses relative  mapping function base on specification shown as on the table below.


| Mapping | Mapping with parallelisation | Number of parameters |
| :------: | :------: | :------: |
| `purrr:map()` | `furrrr:future_map()` | $n = 1$ |
| `purrr:map2()` | `furrrr:future_map2()` | $n = 2$ | 
| `purrr:pmap()` | `furrrr:future_pmap()` | $n \geq 3$ | 

`options = furrr_options(seed = TRUE)` is for reproducible random number generation (RNG) processes. This argument takes control of the RNG process for parallelization and always generates the same numbers according to the given seed. More details can be found by running the command `?furrr_options` in RStudio.


```{r data generation and parallel process}
data_generation <- function(simulation, grid){
  
  if(ncol(grid)==2){
    var1 <- c(unlist(grid[,1]))
    if(cores>1){
      data <- future_map(var1, simulation,
                         .options = furrr_options(seed = TRUE))
    }else{
      data <- map(var1, simulation)
    }
  }
  
  if(ncol(grid)==3){
    var1 <- c(unlist(grid[,1]))
    var2 <- c(unlist(grid[,2]))
    if(cores>1){
      data <- future_map2(var1, var2, simulation,
                          .options = furrr_options(seed = TRUE))
    } else{
      data <- map2(var1, var2, simulation)
    }
  } 
  
  if(ncol(grid)==4){
    var1 <- c(unlist(grid[,1]))
    var2 <- c(unlist(grid[,2]))
    var3 <- c(unlist(grid[,3]))
    list1 <- list(var1,var2,var3)
    if(cores>1){
      data <- future_pmap(list1, .f=simulation,
                          .options = furrr_options(seed = TRUE))
    }else{
      data <- pmap(list1, .f=simulation)
    }
  }
  
  return(data)
}
```

Monte Carlo simulations can become very demanding in terms of computing time. In that case, parallel processing may be used to reduce the time of the simulation. In parallel processes, each process is executed simultaneously but independently. Interconnections are proceeded through communication channel [@Czech_2017].

`data_generation()` automatically used the proper function from the `furrrr`-package. If the user specified more than one core. Otherwise, the function will stick to the respective mapping function from the `purrr`-package.


**Example to `data_generation()`:**
The example below demonstrates the data generation process with  Poisson distribution without parallelisation. The advantage in computation time with parallelisation will be discussed in Chapter 3.

```{r test data_generation()}
cores <- 1


param_list2 <- list(c("n", 10, 20, 10)
                  ,c("lambda", 0.5, 1, 0.5))



grid2 <- create_grid(param_list2, nrep=1)
sim1 <- data_generation(simulation=rpois, grid=grid2)


names(sim1) <- c("n=10, lamda = 0.5","n=10, lamda = 1",
                 "n=20, lamda = 0.5","n=20, lamda = 1")
grid2
sim1
 
```

`sim1` contains the generated data by `data_generation()` with `grid2` and Poisson distribution as input.
The function below shows the application of `data_generation()` to uniform and Poisson distribution:

```{r, dg application}
# Application to uniform distribution
param_list_runif <- list(c("n", 10, 30, 10)
                         ,c("min", 0, 0, 0)
                         ,c("max", 1, 1, 0))


grid_unif <- create_grid(param_list_runif, nrep=3)
head(data_generation(simulation=runif, grid=grid_unif),1)

```

As explained before, the data is stored in a list, where data points for each set of parameters are stored under a different variable. The first variable (saved under `$n1`) is based on 10 draws from a uniform distribution with $\mathcal{U}_{[0,2]}$. Each variable in the list relates to a row in the parameter grid.


## summary_function()

So far, the paper explains how to create *raw* data with the functions `create_grid()` and  `data_generation()` using chosen probability distribution. This chapter introduces `summary_function()` which applies a user-defined function to create summary statistic on the raw data generated before.
The function uses a sapply-loop to apply the user-defined summary_function()(`sum_fun`)(such as mean()) on each element of the list containing the raw data. Results are stored in a $(nrow(grid ) X 1)$-dimensional matrix, which is combined with the parameter grid in the next step.


```{r summary_function}

summary_function <- function(sum_fun, data_input){
  
  count <- length(data_input)
  summary_matrix <- matrix(nrow=count, ncol=1)
  
  for(i in 1:count){
    input <- list(data_input[[i]])
    output <- sapply(sum_fun, do.call, input)
    summary_matrix[i] <- output
  }

  colnames(summary_matrix) <- sum_fun
  return(summary_matrix)
}
```

**`summary_function()` Example:**

For this example, data is generated using the normal distribution, with the respective parameters n, $\mu$ and $\sigma$ specified in the parameter list. The list containing the raw data created by `data_generation()` is stored in the variable `test_data`, which is the data input for `summary_function()`. The arithmetic mean, with base R `mean()` function, is supplied as the second input to calculate the mean of all the data points created under a unique set of parameters. The output relates exactly to each row of the used parameter grid. Later, these summary statistics  R merged with the parameter grid in a data frame.


```{r summary test}

param_list3 <- list(c("n", 10, 20, 10)
                    ,c("mu", 1, 2, 0.25)
                    ,c("sd", 0.5, 1, 0.1))

grid_test <- create_grid(param_list3, nrep=3)
test_data <- data_generation(simulation=rnorm, grid=grid_test)
summary_data <- summary_function(sum_fun=list("mean"),
                                 data_input=test_data)
head(summary_data)
nrow(summary_data)
```


## create_array_function()

?????????????Even though the example above includes a relatively small parameter grids, the simulation returns 180 summarize data points. In `main_function()`, the results from the previous step get merged with the parameter grid into one data frame. This way of storing the data allows user to apply further data wrangling processes. A multidimensional array is more suitable for printing the output in a tidy and clear way. `create_array_function()` takes all relevant data from the previous steps (parameter grid and the results of the Monte Carlo simulation) and transforms it into an array with the correct dimensions.???????????????

```{r, array}
create_array_function <- function(comb, parameters, nrep){
  storage <- list()
  name_vec <- c()
  
  for(i in 1:length(parameters)){ 
    a <- as.numeric(parameters[[i]][[2]])
    b <- as.numeric(parameters[[i]][[3]])
    c <- as.numeric(parameters[[i]][[4]])
    output <- seq(from=a, to=b, by=c)
    storage[[i]] <-  output
    name_vec[i] <- parameters[[i]][[1]] 
  }
  
  
  matrix.numeration <-  paste("rep","=", 1:nrep, sep = "")
  
  if(length(parameters)==1){
    comb_ordered <-  comb %>% arrange(comb[,2])
    seq1 <- c(unlist(storage[1]))
    
    row.names <- paste(name_vec[1],"=",seq1, sep = "")
    
    dimension_array <- c(length(seq1), nrep)
    dim_names_list <- list(row.names, matrix.numeration)
  }
  
  if(length(parameters)==2){
    comb_ordered <-  comb %>% arrange(comb[,2])  %>% arrange(comb[,3])
    seq1 <- c(unlist(storage[1]))
    seq2 <- c(unlist(storage[2]))
    
    row.names <- paste(name_vec[1],"=",seq1, sep = "")
    column.names <-  paste(name_vec[2],"=",seq2, sep = "")
    
    dimension_array <- c(length(seq1), length(seq2), nrep)
    dim_names_list <- list(row.names, column.names, matrix.numeration)
  }
  
  if(length(parameters)==3){
    comb_ordered <-  comb %>% arrange(comb[,2])  %>% 
      arrange(comb[,3]) %>% arrange(comb[,4]) 
    seq1 <- c(unlist(storage[1]))
    seq2 <- c(unlist(storage[2]))
    seq3 <- c(unlist(storage[3]))
    
    row.names <- paste(name_vec[1],"=",seq1, sep = "")
    column.names <-  paste(name_vec[2],"=",seq2, sep = "")
    matrix.names1 <-  paste(name_vec[3],"=",seq3, sep = "")
    
    dimension_array <- c(length(seq1), length(seq2), length(seq3), nrep)
    dim_names_list <- list(row.names, column.names, 
                           matrix.names1, matrix.numeration)
    
  }
  
  
  array1 <- array(comb_ordered[,ncol(comb)] 
                  , dim = dimension_array
                  , dim_names_list)
  return(array1)
}

```

In order to test `create_array_function()`, we need to set up an altered version of `main_function()`. The `main_function()` is discussed in the next chapter. Also, a slightly modified version of the example using the `rnorm()` is used, where the parameter grid spans over a larger sequence


  

**Example to `create_array_function` :**
```{r, array ex}

# PREP TEST `create_array_function`
main_function_array_test <-  function(parameters #list of parameters
                                      , nrep #number of repetitions
                                      , simulation #data genereation
                                      , sum_fun){ #summary statistics
  
  grid <- create_grid(parameters, nrep) #Step 1: create grid
  raw_data <- data_generation(simulation, grid) #Step 2: simlate data
  summary <- summary_function(sum_fun, data_input=raw_data) #Step 3: Summary statistics
  comb <- cbind(grid, summary) #Step 4: Combine resuluts with parameters
  array_1 <- create_array_function(comb, parameters, nrep) #Step 5: Create array
  
  return(comb)
}





param_list3x <- list(c("n", 10, 30, 10)
                     ,c("mu", 0, 6, 1)
                     ,c("sd", 1, 2, 1))

comb1 <- main_function_array_test(parameters=param_list3x
                                  , nrep = 2
                                  , simulation = rnorm
                                  , sum_fun="mean")

array_test <- create_array_function(comb=comb1,
                                    parameters=param_list3x, nrep=2)

array_test
```

Under these specifications, the data was transformed into a 4-dimensional array. Sample size n and the mean $\mu$ are the variables at the side of each row and column. Also, it was specified that two repetitions are done for each set of parameters, while two values ($\sigma = \{1, 2\}$) were given for the standard deviation. The output contains the correct amount of tables (4). This output style allows the user  easily to oversee a wide variety of different parameter constellations.
???????????????Through `create_array_function()`, the user can obtain all combinations corresponding to each grid, the number of repetitions and the summary function that user require, such as `mean` in this case.????????????????????????????


## average_function()

The example above used only two repetitions for each parameter constellation to keep the output simple. However, in practice the user would probably repeat the simulations over a higher number of repetitions, which would also drastically increase the size of the array. `average_function()` calculates the average over all repetitions and stores in an array with a dimension, that is reduced by one since the dimension for repetitions is not relevant anymore.


```{r}

average_function <- function(grid_for_avg, summary, nrep){
  grid_for_avg <- grid_for_avg[-ncol(grid_for_avg)] #remove column for reps
  n_rows <- nrow(grid_for_avg)
  n_col <- ncol(grid_for_avg)
  
  for(i in 1:n_rows){
    start <- 1 + (i-1)*nrep
    end <- i*nrep
    grid_for_avg[i, n_col+1] <- mean(summary[start:end, ])
  }
  
  grid_plus_mc <- data.frame(grid_for_avg)
  
  colnames(grid_plus_mc)[n_col+1] <- "avg"
  
  return(grid_plus_mc)
  
}
```





## output_function()

`output_function()`  is the last part of pre-process. This function takes results and parameters of the Monte Carlo simulation as inputs and converts them into an tidy output format. Thus, user can obtain the tidy form of a simulation summary.

`array_1`, `average_over_reps`, `parameters`, `cores`, `simulation`,
`nrep`, `cpt`  are used as input parameters in the `output_function()`.
`cpt` variable for the execution time of the simulation. More detail will be addressed in the next chapter.

Regarding the structure of `output_function()`, (1) `out` is an empty list for storing simulation results, averaged results and summary. (2) `Eco` is the class name of `out`. The class should be assigned to visualize simulation results with `ggplot2` methods. (3) Results from `array_1` and `average_over_reps` are assigned as `out$results` and `out$average` respectively. (4) These results have the classes with their own names: `out$results` and `out$average`, thereby preventing future error when visualizing simulation results with `ggplot2` methods. `ggplot2` methods require specific classes, such as `data.frame`. (5) `cat` is useful for producing output in user-defined functions. It converts its arguments to character vectors, concatenates them to a single character vector, appends the given `sep`$ =$ string(s) to each element and then outputs them [@rdocumentation]. (6) The neat output is returned at the end.


```{r echo=TRUE}


output_function <- function(array_1,average_over_reps,parameters,cores,simulation,
                            nrep,cpt){
  # (1)
  out <- list() 
  # (2) 
  class(out) <- "Eco" 
  # (3)
  out$results <- array_1 
  out$average <- average_over_reps 
  # (4)
  class(out$average) <- c("Eco",class(out$average))
  class(out$results) <- c("Eco",class(out$results))
  # (5)
  if(cores>1){
    parallel = "Multisession"
  } else {       
    parallel = "Sequential"
  }
  text <-  cat("\n",
           "Repetition(nrep)      : ",nrep,"\n\n",
           "Parallelization Type  : ",parallel,"\n\n",
           "Number of Cores Used in  Parallelization : ",cores,
           " out of",detectCores(),"\n\n",
           "Input Parameters : ",paste(parameters),"\n\n",
           "Simulation Length :",length(array_1),"\n",
           "Minumum :",min(array_1),"\n",
           "Maximum :",max(array_1),"\n",
           "Mean    :", mean(array_1),"\n",
           "Median  :",median(array_1),"\n\n",
           "Execution Time of Monte Carlo Simulation",
           as.numeric(cpt),"secs \n\n",
           "Name of The Class :",class(out))
  # (6)
  return(out)
}

```
`output_function()` represents the output of the main Monte Carlo simulation function in the next chapter.



# main_function() : Monte Carlo simulation function

The `main_function()` is built-up by the helper functions  introduced in chapter 2 and includes additional arguments to  combine different steps of the helper functions. (1) `parameters` is a parameter list, `nrep` is the number of repetitions, `simulation()` is a data generation process, `sum_fun` is summary function that user define , `seed set` is for reproducibility of the simulation and `cores` is the number of cores that exist in CPU. (2) `if and else` commands check how many cores are used in the `main_function()` and whether `cores` are more than the maximum number of cores. By using `detectCores()` from the `parallel` package, the maximum number of cores in CPU is saved in the variable `max.cores`. (3) Setting the seed is an important step to get the same result for randomization. `main_function()` either takes the seed that user decide or creates the random seed when there is no seed provided. `sample.int()` generates a random number and uses it as a seed for reproducibility of the simulation. (4) `Sys.time()` function confirms the execution time of the simulation. `startTime` and `endTime` save the simulation's starting time and ending time, respectively. The difference between them is the execution time `cpt`, which is also included in the summary statistics. (5) `plan()` is used to parallelise  the simulation. `plan()` has two arguments, “sequential” or "multisession”. "Sequential" runs non-parallel processing with one core. On the other hand, user can define the number of cores with the "Multisession" argument. Lastly, (6) `main_function()` produces tidy summary statistics with `output_function()` in chapter 2.


```{r}
#(1)
main_function <-  function(parameters 
                           , nrep 
                           , simulation 
                           , sum_fun 
                           ,seed = NULL
                           ,cores=NULL){
 
    
  #(2)
  max.cores <- detectCores()
  if(cores>max.cores){
    stop("Number of Cores cannot be bigger than total number of cores")
  }
  #(3)
  if(!is.null(seed)) {
    set.seed(seed)}
  else {
    warning("No seed provided!", call. = FALSE)
    seed <- sample.int(10000, 1)
    set.seed(seed)
    message("Random seed = ", seed, "\n")} 
  
  #(4) and (5)
  startTime <- Sys.time()#Starting time 
  
  
  
  grid <- create_grid(parameters, nrep) 
  
  if(cores > 1){
    plan(multisession,workers = cores)
  } else{
    plan(sequential)
  }
  suppressMessages(raw_data <- data_generation(simulation, grid))
  
  summary <- summary_function(sum_fun, data_input=raw_data) 
  
  average_over_reps <- average_function(grid_for_avg=create_grid(parameters, 1),
                                        summary, nrep)
  
  comb <- cbind(grid, summary) 
  
  array_1 <- create_array_function(comb, parameters, nrep) 
  
  endTime <- Sys.time()
  
  cpt <- endTime - startTime
  
  #(6)
  summary_1 <- output_function(array_1,
                               average_over_reps,
                               parameters,cores,
                               simulation,nrep,cpt)
  
return(summary_1)
}

```


The tests below present the performance of `main_function()` with normal distribution.

## Test 1 : Summary performance

```{r, Testing main function}
param_list3x <- list(c("n", 10, 100, 10)
                     ,c("mu", 0, 10, 1)
                     ,c("sd", 0, 5, 1))

test_me <- main_function(parameters=param_list3x
              , nrep = 5
              , simulation = rnorm
              , sum_fun="mean"
              , seed=123 
              , cores=1)

```

All summary components, results, average result and simulation summary are clear.


## Test 2 : Visualization performance

* Visualizing average
```{r,ggplot2}
ggplot(test_me$average,aes(x=avg,y=n))+geom_line()

```

* Visualizing average by using `facet_grid()`
```{r,facet}
ggplot(test_me$average,aes(x=avg))+facet_grid(n~.)+
  geom_density()

```
As given graphs above, the simulation result works well with `ggplot2` methods.

# Examples

## Comparing the execution times of OLS and GLS simulations
As an example of Monte Carlo simulation, OLS and GLS coefficients $\beta$ are simulated with and without parallelisation to compare the execution time.
```{r,ols-gls}
ols_f <- function(n,mu,sd){
  e <- rnorm(n,mu,sd)
  x <- runif(n)
  y <- 0.5*x + e
  ols.hat <- t(x) %*% y / t(x)%*%x
  return("ols"=ols.hat)}

gls_f <- function(n,mu,sd){
  e <- rnorm(n,mu,sd)
  x <- runif(n)
  y <- 0.5*x + e
  v.inv <- diag(1/(1:n))
  c <- chol(v.inv)
  cy <- c %*% y
  cx <- c %*% x
  gls_hat <- t(cx) %*% cy / t(cx)%*%cx
  return("gls"=gls_hat)
  
  param_list <- list(c("n",100,1000,100),c("mu",0,1,0.25),c("sd",1,2,.5))
}
```
As shown above, simple OLS and GLS functions are defined to find $\beta$ coefficients. However, the execution time of the GLS  would be much longer than OLS since The Cholesky Decomposition `chol()` is applied to  the GLS function.


**OLS simulation without parallel processing:**

```{r , ols}
 param_list <- list(c("n",100,1000,100),c("mu",0,1,0.25),c("sd",1,2,.5))
ols <- main_function(parameters = param_list,
                     nrep=5,
                     simulation = ols_f,
                     sum_fun="mean",
                     seed=123,
                     cores=1)

```
The total execution time of OLS simulation is 8.46 seconds when only one core is used.


**OLS simulation with parallel processing:**

```{r, parallel ols}
ols <- main_function(parameters = param_list,
                     nrep=5,
                     simulation = ols_f,
                     sum_fun="mean",
                     seed=123,
                     cores=4)
```
The total execution time of OLS simulation is 12 seconds when only four core is used.


**GLS with parallel processing:** 
```{r,gls in parallel}
gls <- main_function(parameters = param_list,
                     nrep=5,
                     simulation = gls_f,
                     sum_fun="mean",
                     seed=123,
                     cores=4)

```


**GLS without parallel processing:**

```{r,gls}
gls <- main_function(parameters = param_list,
                     nrep=5,
                     simulation = gls_f,
                     sum_fun="mean",
                     seed=123,
                     cores=1)

```
As seen in the summary part, the total execution time of the simulation took 36.35 seconds which also proves that the parallel process works well. Therefore, execution times of the simulation might differ on other computers.


## Visualisation 

**Visualizing MSE(Mean Square Error) in OLS and GLS simulations:**

MSE is calculated by `out$average$mse` for each simulation.

```{r}
gls$average <-  gls$average %>% mutate(mse =(2-avg)^2 )
ols$average <-  ols$average %>% mutate(mse =(2-avg)^2 )

ols$average %>% ggplot(aes(x=avg,y=mu,col="OLS"))+
  facet_grid(n~mean(mse))+geom_line()+
 geom_line(data=gls$average,aes(x=avg,y=mu,col="GLS"))+
  scale_color_manual(name = "Estimation",
                     values = c("OLS" = "blue", "GLS" = "red"))

```
\pagebreak

**Density graph of MSE of $\beta$ in OLS and GLS simulations**
```{r}
ggplot(ols$average,aes(x=mse,col="OLS"))+facet_grid(n~.)+
  geom_density()+
  geom_density(data=gls$average,aes(x=avg,col="GLS"))+
  scale_color_manual(name = "Estimation", 
                     values = c("OLS" = "blue", "GLS" = "red"))
```


\pagebreak

# Conclusion

The above section illustrates the power of the implemented model and provides the fairly easy-to-use tool that still allows for various specifications in terms of used parameters, data generation processes and summary functions. Researchers, who use Monte Carlo studies regularly may save much time using a tool like this in the long run.

By nature, there may be cases, where the implementation does not satisfy the user's needs to the fullest, but for a wide variety of examples it is showed, it worked well and served the over all purpose. Furthermore, the functional programming approach allows for easy and flexible adjustments in case the use of the functions should be expanded, such as if a grid of more than 3 (or 4?) parameters is needed.???????????????????????????????????????????????????

\pagebreak

# Contributions

|   | Alexander Langnau | Öcal Kaptan | Sunyoung Ji |
| :------: | :------: | :------: | :------: |
| Planning | 0 | 0 | 0 |
| Create_grid | 0 | 0 | 0 |
| Data_generation  | 0 | 0 | 0 | 
| Summary_function  | 0 | 0 | 0 | 
| Create_array_function  | 0 | 0 | 0 | 
| Average_function  | 0 | 0 | 0 | 
| Output_function  | 0 | 0 | 0 | 
| ggplot2 part   | 0 | 0 | 0 | 
| Parallelisation  | 0 | 0 | 0 | 
| Formatting  | 0 | 0 | 0 | 
| Writing the report  | 0 | 0 | 0 | 
| Proof-reading  | 0 | 0 | 0 | 











\pagebreak

# References

Adrian G. Barbu, Song Chun Zhu, "Monte Carlo Methods", Springer Singapore, 2020, pp.1-4, doi:https://doi.org/10.1007/978-981-13-2971-5
\

Czech, Z. "In Introduction to Parallel Computing", Cambridge University Press, 2017, pp. 1-34, doi:10.1017/9781316795835.002

rdocumentation.org, DataCamp, "cat: Concatenate and Print", url:https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/cat


\pagebreak

# Appendix

## Output of  the OLS simulation
```{r echo=FALSE}
ols
```


## Output of the GLS simulation

```{r echo=FALSE}
gls
```
